from .RecommendGraph import rs_builder
from .SearchWebGraph import search_builder 
from .SearchWikiGraph import ws_builder
from .memoryCcollection import PostgresStore
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field
from IPython.display import Image, display
from langchain_core.messages import  SystemMessage
from langchain.schema import HumanMessage, SystemMessage

from langgraph.graph import MessagesState
import operator
import uuid
from langchain_core.runnables.config import RunnableConfig
from typing import Annotated
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from trustcall import create_extractor
from langchain_core.messages import merge_message_runs
from fastapi import FastAPI
load_dotenv()
from langchain_core.chat_history import InMemoryChatMessageHistory
from config import DATABASE_URL
chat_history = InMemoryChatMessageHistory()
# Kh·ªüi t·∫°o SQLite connection
app = FastAPI()

llm = ChatOpenAI(model="gpt-4o")
class Memory(BaseModel):
    content: str = Field(description="N·ªôi dung ch√≠nh c·ªßa b·ªô nh·ªõ. V√≠ d·ª•: Ng∆∞·ªùi d√πng th√≠ch ƒÉn m√≥n ph·ªü.")

class EntryGraphState(MessagesState):
    query: str
    context: Annotated[list, operator.add]
    subgraph_name: str
    user_id: int
    
    
trustcall_extractor = create_extractor(
    llm,
    tools=[Memory],
    tool_choice="Memory",
    enable_inserts=True,
)

# --- Tin nh·∫Øn h·ªá th·ªëng cho chatbot ---


TRUSTCALL_INSTRUCTION = """H√£y suy ng·∫´m v·ªÅ cu·ªôc t∆∞∆°ng t√°c sau ƒë√¢y. S·ª≠ d·ª•ng c√°c c√¥ng c·ª• ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ l∆∞u l·∫°i b·∫•t k·ª≥ th√¥ng tin c·∫ßn thi·∫øt n√†o v·ªÅ ng∆∞·ªùi d√πng. 
H√£y s·ª≠ d·ª•ng g·ªçi c√¥ng c·ª• song song ƒë·ªÉ x·ª≠ l√Ω vi·ªác c·∫≠p nh·∫≠t v√† ch√®n d·ªØ li·ªáu c√πng l√∫c."""

# --- H√†m g·ªçi model d·ª±a tr√™n tr·∫°ng th√°i v√† b·ªô nh·ªõ ---
def call_model(state: EntryGraphState, config: RunnableConfig):
    print("Config received in call_model:", config)
    store = config["configurable"]["store"]   # L·∫•y store t·ª´ config
    user_id = config["configurable"]["user_id"]
    namespace = ("memories", user_id)
    memories = store.search(namespace)

    print(f"üìå Memories fetched from store for user_id {user_id}:")
    for mem in memories:
        print(f"üß† {mem.value['content']}")

    info = "\n".join(f"- {mem.value['content']}" for mem in memories)


    print("Response from LLM:", info)
    return {"context": [info]}

# H√†m write_memory c≈©ng t∆∞∆°ng t·ª±
def write_memory(state: EntryGraphState, config: RunnableConfig):
    store = config["configurable"]["store"] 
    user_id = config["configurable"]["user_id"]
    namespace = ("memories", user_id)

    # L·ªçc tin nh·∫Øn ng∆∞·ªùi d√πng
    user_messages = [msg for msg in state['messages'] if isinstance(msg, HumanMessage)]

    # Ch·ªâ ghi nh·ªõ khi ƒë√£ ƒë·ªß 5 tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng
    if len(user_messages) < 5:
        return  # Kh√¥ng l√†m g√¨ n·∫øu ch∆∞a ƒë·ªß

    last_5_messages = state['messages'][-5:]
    # L·∫•y tin nh·∫Øn ƒë·∫ßy ƒë·ªß ƒë·ªÉ t√≥m t·∫Øt
    updated_messages = list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + last_5_messages))

    existing_items = store.search(namespace)
    tool_name = "Memory"
    existing_memories = ([
        (existing_item.key, tool_name, existing_item.value)
        for existing_item in existing_items
    ] if existing_items else None)

    # G·ªçi TrustCall ƒë·ªÉ t√≥m t·∫Øt v√† tr√≠ch xu·∫•t memory
    result = trustcall_extractor.invoke({
        "messages": updated_messages,
        "existing": existing_memories
    })

    # Ghi v√†o store
    for r, rmeta in zip(result["responses"], result["response_metadata"]):
        store.put(
            namespace,
            rmeta.get("json_doc_id", str(uuid.uuid4())),
            r.model_dump(mode="json")
        )
    
def controller(state):
    """ Controller to decide which subgraph to use """
    prompt_template = """B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o c·ªßa web du l·ªãch MonkeyDvuvi, m·ªôt website h·ªó tr·ª£ ƒë·∫∑t ph√≤ng kh√°ch s·∫°n, du thuy·ªÅn tr·ª±c tuy·∫øn. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
    c√°c ki·∫øn th·ª©c v·ªÅ c√°c ƒë·ªãa ƒëi·ªÉm du l·ªãch, nh√† h√†ng, kh√°ch s·∫°n, du thuy·ªÅn t·∫°i Vi·ªát Nam, ngo√†i ra c√≥ th·ªÉ g·ª£i √Ω tour v√† c√°c l·ªãch tr√¨nh chi ti·∫øt ph√π h·ª£p v·ªõi nhu c·∫ßu ng∆∞·ªùi d√πng.
    D·ª±a tr√™n c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng, h√£y quy·∫øt ƒë·ªãnh subgraph n√†o n√™n ƒë∆∞·ª£c k√≠ch ho·∫°t ƒë·ªÉ Agent c√≥ th·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi t·ªët nh·∫•t cho ng∆∞·ªùi d√πng.
    H√£y tr·∫£ l·ªùi v·ªõi m·ªôt trong c√°c subgraph sau: Recommendation System, Search Wikipedia v√† Search Web.
    N·∫øu c√¢u h·ªèi ƒë∆∞a v√†o li√™n quan t·ªõi y√™u c·∫ßu x√¢y d·ª±ng tour du l·ªãch, h·ªèi ƒë√°p v·ªÅ nh√† h√†ng, kh√°ch s·∫°n c·ª• th·ªÉ, h√£y g·ªçi t·ªõi Recommendation System.
    N·∫øu c√¢u h·ªèi ƒë∆∞a v√†o li√™n quan t·ªõi y√™u c·∫ßu t√¨m ki·∫øm th√¥ng tin t·ª´ wikipedia, h√£y g·ªçi t·ªõi Search Wikipedia.
    N·∫øu c√¢u h·ªèi ƒë∆∞a v√†o li√™n quan t·ªõi y√™u c·∫ßu t√¨m ki·∫øm th√¥ng tin t·ª´ google, h√£y g·ªçi t·ªõi Search Web.
    N·∫øu kh√¥ng c√≥ subgraph n√†o ph√π h·ª£p, h√£y g·ªçi t·ªõi answer.
    L∆ØU √ù: CH·ªà ƒê∆ØA RA M·ªòT TRONG C√ÅC GI√Å TR·ªä N√ÄY, KH√îNG ƒê∆ØA RA C√ÅC GI√Å TR·ªä KH√ÅC.
    H√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† s√∫c t√≠ch.
    ƒê√¢y l√† c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {query}"""
    print("The prior messages are:" , state['messages']) 
    messages = llm.invoke(prompt_template.format(query=state["query"]))
    if messages.content == "Recommendation System":
        print("Recommendation System")
        return {"subgraph_name": "Recommendation System"}
    elif messages.content == "Search Wikipedia":
        print("Search Wikipedia")
        return {"subgraph_name": "Search Wikipedia"}
    elif messages.content == "Search Web":
        print("Search Web")
        return {"subgraph_name": "Search Web"}
    else:
        return {"subgraph_name": "answer"}

def answer(state):
    prompt_template = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o c·ªßa web du l·ªãch MonkeyDvuvi, m·ªôt website h·ªó tr·ª£ ƒë·∫∑t ph√≤ng kh√°ch s·∫°n, du thuy·ªÅn tr·ª±c tuy·∫øn. 
B·∫°n h√£y tr·∫£ l·ªùi c√°c c√¢u h·ªèi t∆∞∆°ng t√°c v·ªõi ng∆∞·ªùi d√πng m·ªôt c√°ch t·ª± nhi√™n v√† th√¢n thi·ªán, n·∫øu c·∫ßn thi·∫øt h√£y vui t√≠nh.

Ng·ªØ c·∫£nh hi·ªán t·∫°i:
{state['context']}
H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n ng·ªØ c·∫£nh hi·ªán t·∫°i v√† c√°c th√¥ng tin ƒë√£ c√≥ trong b·ªô nh·ªõ.
"""
    print("The prior messages are:", state['messages']) 
    response = llm.invoke([SystemMessage(content=prompt_template)] + state['messages'])
    return {"messages": response}


def condition_tools(state):
    if state['subgraph_name'] == "Recommendation System":
        return "Recommendation System"
    elif state['subgraph_name'] == "Search Wikipedia":
        return "Search Wikipedia"
    elif state['subgraph_name'] == "Search Web":
        return "Search Web"
    elif state['subgraph_name'] == "answer":
        return "answer" 
    
across_thread_memory = PostgresStore(DATABASE_URL) 

builder = StateGraph(EntryGraphState)
builder.add_node("call_model", call_model)
builder.add_node("write_memory", write_memory)
builder.add_node("controller", controller)
builder.add_node("Recommendation System", rs_builder.compile())
builder.add_node("Search Wikipedia", ws_builder.compile())
builder.add_node("Search Web", search_builder.compile())
builder.add_node("answer", answer)
builder.add_edge(START, "call_model")
builder.add_edge("call_model", "controller")
builder.add_conditional_edges(
    "controller",
    condition_tools,
    {
        "Recommendation System": "Recommendation System",
        "Search Wikipedia": "Search Wikipedia",
        "Search Web": "Search Web",
        "answer": "answer"
    }
)
builder.add_edge("Recommendation System", "write_memory")
builder.add_edge("Search Wikipedia", "write_memory")
builder.add_edge("Search Web", "write_memory")
builder.add_edge("answer", "write_memory")
builder.add_edge("write_memory", END)
graph = builder.compile()